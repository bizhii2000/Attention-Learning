{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.mpe import simple_spread_v3\n",
    "import time\n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input batch size 32 doesn't match hidden0 batch size 96",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 98\u001b[0m\n\u001b[0;32m     96\u001b[0m obs_batch, action_batch, reward_batch, next_obs_batch, mask_batch, done_batch \u001b[39m=\u001b[39m memory\u001b[39m.\u001b[39msample(args\u001b[39m.\u001b[39mbatch_size)\n\u001b[0;32m     97\u001b[0m sample_batch \u001b[39m=\u001b[39m (obs_batch, action_batch, reward_batch, next_obs_batch, mask_batch, done_batch)\n\u001b[1;32m---> 98\u001b[0m critic_loss, policy_loss \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mupdate_parameters(sample_batch, args\u001b[39m.\u001b[39;49mbatch_size, updates, train_policy)\n\u001b[0;32m     99\u001b[0m cl_mean \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(np\u001b[39m.\u001b[39masarray(critic_loss))\n\u001b[0;32m    100\u001b[0m pl_mean \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(np\u001b[39m.\u001b[39masarray(policy_loss))\n",
      "File \u001b[1;32mc:\\Users\\nimaz\\Desktop\\Sandbox\\RL\\SMAC\\continuous_Qmix\\train.py:112\u001b[0m, in \u001b[0;36mAgentsTrainer.update_parameters\u001b[1;34m(self, samples, batch_size, updates, train_policy)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 112\u001b[0m     action_next, log_p_next, _, actors_target_h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactors_target\u001b[39m.\u001b[39;49msample(obs_next_slice, actors_target_h)\n\u001b[0;32m    113\u001b[0m     qs_next, critics_target_h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcritics_target(obs_next_slice, action_next, critics_target_h)\u001b[39m# - self.alpha * log_p_next\u001b[39;00m\n\u001b[0;32m    114\u001b[0m     qs_next \u001b[39m=\u001b[39m qs_next\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mna)\n",
      "File \u001b[1;32mc:\\Users\\nimaz\\Desktop\\Sandbox\\RL\\SMAC\\continuous_Qmix\\model.py:49\u001b[0m, in \u001b[0;36mRNNGaussianPolicy.sample\u001b[1;34m(self, state, h)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msample\u001b[39m(\u001b[39mself\u001b[39m, state, h):\n\u001b[1;32m---> 49\u001b[0m     mean, log_std, h_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(state, h)\n\u001b[0;32m     50\u001b[0m     std \u001b[39m=\u001b[39m log_std\u001b[39m.\u001b[39mexp()\n\u001b[0;32m     51\u001b[0m     normal \u001b[39m=\u001b[39m Normal(mean, std)\n",
      "File \u001b[1;32mc:\\Users\\nimaz\\Desktop\\Sandbox\\RL\\SMAC\\continuous_Qmix\\model.py:40\u001b[0m, in \u001b[0;36mRNNGaussianPolicy.forward\u001b[1;34m(self, state, h)\u001b[0m\n\u001b[0;32m     38\u001b[0m     h_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrnn(x)\n\u001b[0;32m     39\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 40\u001b[0m     h_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrnn(x, h)\n\u001b[0;32m     42\u001b[0m mean \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmean(h_out)\n\u001b[0;32m     43\u001b[0m log_std \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_std(h_out)\n",
      "File \u001b[1;32mc:\\Users\\nimaz\\anaconda3\\envs\\deep\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\nimaz\\anaconda3\\envs\\deep\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:1327\u001b[0m, in \u001b[0;36mGRUCell.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m   1324\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1325\u001b[0m     hx \u001b[39m=\u001b[39m hx\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_batched \u001b[39melse\u001b[39;00m hx\n\u001b[1;32m-> 1327\u001b[0m ret \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mgru_cell(\n\u001b[0;32m   1328\u001b[0m     \u001b[39minput\u001b[39;49m, hx,\n\u001b[0;32m   1329\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight_ih, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight_hh,\n\u001b[0;32m   1330\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_ih, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_hh,\n\u001b[0;32m   1331\u001b[0m )\n\u001b[0;32m   1333\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_batched:\n\u001b[0;32m   1334\u001b[0m     ret \u001b[39m=\u001b[39m ret\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input batch size 32 doesn't match hidden0 batch size 96"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np \n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "from buffer import ReplayMemory\n",
    "from train import AgentsTrainer\n",
    "max_taining_steps = 600\n",
    "start_steps = max_taining_steps//5\n",
    "parser = argparse.ArgumentParser(description='PyTorch QMIX Args')\n",
    "parser.add_argument('--scenario', type=str, default='simple_spread', help='name of the scenario script')\n",
    "parser.add_argument('--num_episodes', type=int, default=max_taining_steps, help='number of episodes for training')\n",
    "parser.add_argument('--max_episode_len', type=int, default=25, help='maximum episode length')\n",
    "parser.add_argument('--policy_lr', type=float, default=1e-4, help='learning rate for policies')\n",
    "parser.add_argument('--critic_lr', type=float, default=1e-2, help='learning rate for critics')\n",
    "parser.add_argument('--alpha', type=float, default=0.0, help='policy entropy term coefficient')\n",
    "parser.add_argument('--tau', type=float, default=0.05, help='target network smoothing coefficient')\n",
    "parser.add_argument('--gamma', type=float, default=0.99, help='discount factor (default: 0.99)')\n",
    "parser.add_argument('--seed', type=int, default=123, help='random seed (default: 123)')\n",
    "parser.add_argument('--batch_size', type=int, default=32, help='batch size (default: 16)') # episodes\n",
    "parser.add_argument('--hidden_dim', type=int, default=64, help='network hidden size (default: 256)')\n",
    "parser.add_argument('--start_steps', type=int, default=start_steps, help='steps before training begins')\n",
    "parser.add_argument('--target_update_interval', type=int, default=1, help='tagert network update interval')\n",
    "parser.add_argument('--updates_per_step', type=int, default=1, help='network update frequency')\n",
    "parser.add_argument('--replay_size', type=int, default=50000, help='maximum number of episodes of replay buffer')\n",
    "parser.add_argument('--cuda', action='store_false', help='run on GPU (default: False)')\n",
    "parser.add_argument('--render', action='store_true', help='render or not')\n",
    "sys.argv=['']\n",
    "args = parser.parse_args()\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "# TensorboardX\n",
    "\n",
    "\n",
    "# Load environment\n",
    "N = 3\n",
    "env = simple_spread_v3.env(N=N, local_ratio=0.5, max_cycles=args.max_episode_len, continuous_actions=True,render_mode='human')\n",
    "\n",
    "\n",
    "# Homogeneity check\n",
    "obs_shape = 10\n",
    "action_shape = 1\n",
    "\n",
    "memory = ReplayMemory(args.replay_size, args.max_episode_len, N, obs_shape, action_shape)\n",
    "\n",
    "trainer = AgentsTrainer(N, obs_shape, action_shape, args)\n",
    "total_numsteps = 0\n",
    "updates = 0\n",
    "\n",
    "# reward_bias = 30.\n",
    "train_policy = False\n",
    "for i_episode in itertools.count(1):\n",
    "    episode_reward = 0.0 # sum of all agents\n",
    "    step_within_episode = 0\n",
    "\n",
    "    env.reset()\n",
    "    done = False\n",
    "    terminated = False\n",
    "\n",
    "    while not done and not terminated:\n",
    "        observation = np.random.normal(loc = 0, scale = 1, size=(N, obs_shape))\n",
    "        action_list = np.random.normal(loc = 0, scale = 1, size=(N, action_shape))\n",
    "\n",
    "        # interact with the environment\n",
    "        # new_obs_list, reward_list, done_list, _ = env.step(np.array(action_list))\n",
    "        reward = np.random.normal(loc = 0, scale = 1)\n",
    "        new_observation = np.random.normal(loc = 0, scale = 1, size=(N, obs_shape))\n",
    "        new_obs_list = env.state()\n",
    "        done = np.random.binomial(n=1,p=0.05)\n",
    "        \n",
    "        total_numsteps += 1\n",
    "        step_within_episode += 1\n",
    "        terminated = (step_within_episode >= args.max_episode_len)\n",
    "\n",
    "        # replay memory filling\n",
    "        memory.push(observation, action_list, reward, new_observation, 1. if (done and not terminated) else 0.)\n",
    "        # memory.push(np.asarray(obs_list), np.asarray(action_list), reward_list[0], np.asarray(new_obs_list),\n",
    "        #              1. if done else 0.)\n",
    "        obs_list = new_obs_list\n",
    "\n",
    "        episode_reward += reward\n",
    "\n",
    "    memory.end_episode()\n",
    "    trainer.reset()\n",
    "\n",
    "    if i_episode > 1000:\n",
    "        train_policy = True\n",
    "    if len(memory) > args.batch_size:\n",
    "        for _ in range(args.updates_per_step):\n",
    "            obs_batch, action_batch, reward_batch, next_obs_batch, mask_batch, done_batch = memory.sample(args.batch_size)\n",
    "            sample_batch = (obs_batch, action_batch, reward_batch, next_obs_batch, mask_batch, done_batch)\n",
    "            critic_loss, policy_loss = trainer.update_parameters(sample_batch, args.batch_size, updates, train_policy)\n",
    "            cl_mean = np.mean(np.asarray(critic_loss))\n",
    "            pl_mean = np.mean(np.asarray(policy_loss))\n",
    "            updates += 1\n",
    "    else:\n",
    "        cl_mean = 0. \n",
    "        pl_mean = 0. \n",
    "\n",
    "    # print(\"Episode: {}, total steps: {}, total episodes: {}, reward: {}\".format(i_episode, total_numsteps,\n",
    "    #     step_within_episode, round(episode_reward, 2)))\n",
    "\n",
    "    if i_episode > args.num_episodes:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
